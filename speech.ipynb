{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n",
      "2.0.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "\n",
    "from IPython.display import Audio\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "# SAMPLE_GSM = download_asset(\"tutorial-assets/steam-train-whistle-daniel_simon.gsm\")\n",
    "SAMPLE_WAV = \"D:\\Speech\\clips\\common_voice_en_8981.wav\"\n",
    "# SAMPLE_WAV_8000 = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042-8000hz.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioMetaData(sample_rate=32000, num_frames=161280, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n"
     ]
    }
   ],
   "source": [
    "metadata = torchaudio.info(SAMPLE_WAV)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SAMPLE_WAV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(waveform, sample_rate, output_path=None, width=8, height=4, dpi=100):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    # Decrease the size of the figure\n",
    "    figure, axes = plt.subplots(num_channels, 1, figsize=(width, height), dpi=dpi)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"Waveform\")\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogMelSpec(nn.Module):\n",
    "    def __init__(self, sample_rate=8000, n_mels=128, win_length=160, hop_length=80):\n",
    "        super(LogMelSpec, self).__init__()\n",
    "        self.transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_mels=n_mels,\n",
    "            win_length=win_length, hop_length=hop_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transform(x)  # mel spectrogram\n",
    "        x = np.log(x + 1e-14)  # logarithmic, add small value to avoid inf\n",
    "        return x\n",
    "\n",
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self, rate, policy, freq_mask, time_mask):\n",
    "        super(SpecAugment, self).__init__()\n",
    "        self.time_masking = torchaudio.transforms.TimeMasking(time_mask)\n",
    "        self.freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask)\n",
    "\n",
    "    def forward(self, spec):\n",
    "        spec = self.time_masking(spec)\n",
    "        spec = self.freq_masking(spec)\n",
    "        return spec\n",
    "\n",
    "def plot_log_mel_specgram_torchaudio_featurizer(waveform, sample_rate, n_feats=81, title=\"Log Mel Spectrogram\"):\n",
    "    featurizer = LogMelSpec(sample_rate=sample_rate, n_mels=n_feats, win_length=160, hop_length=80)\n",
    "    log_mel_specgram = featurizer(waveform)\n",
    "    \n",
    "    num_channels, num_features, num_frames = log_mel_specgram.shape  # Update order of dimensions\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 2)  # Create two subplots for original and augmented\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot original spectrogram\n",
    "    for c in range(num_channels):\n",
    "        axes[c][0].imshow(log_mel_specgram[c].detach().numpy(), origin='lower', aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "        axes[c][0].set_xlabel(\"Frame\")\n",
    "        axes[c][0].set_ylabel(\"Mel Frequency Bin\")\n",
    "        axes[c][0].set_title(\"Original Spectrogram\")\n",
    "        if num_channels > 1:\n",
    "            axes[c][0].set_ylabel(f\"Channel {c+1}\")\n",
    "    \n",
    "    # Apply spec augmentation and plot augmented spectrogram\n",
    "    spec_augment = SpecAugment(rate=0.2, policy=2, freq_mask=10, time_mask=15)  # Adjust augmentation parameters\n",
    "    augmented_specgram = spec_augment(log_mel_specgram)\n",
    "    for c in range(num_channels):\n",
    "        axes[c][1].imshow(augmented_specgram[c].detach().numpy(), origin='lower', aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "        axes[c][1].set_xlabel(\"Frame\")\n",
    "        axes[c][1].set_ylabel(\"Mel Frequency Bin\")\n",
    "        axes[c][1].set_title(\"Augmented Spectrogram\")\n",
    "        if num_channels > 1:\n",
    "            axes[c][1].set_ylabel(f\"Channel {c+1}\")\n",
    "    \n",
    "    figure.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "SAMPLE_WAV = r\"D:\\Speech\\clips\\common_voice_en_8981.wav\"\n",
    "waveform, sample_rate = torchaudio.load(SAMPLE_WAV)\n",
    "\n",
    "# Call the plotting function\n",
    "plot_log_mel_specgram_torchaudio_featurizer(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(waveform.numpy()[0], rate=sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
